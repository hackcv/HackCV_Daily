+++
author = "cvley"
categories = ["资讯日报"]
title = "第185期 HackCV 日报"
tags = ["自然语言处理","机器学习","PyTorch"]
date = "2018-04-05"
+++

- [从零开始研究——元学习meta-learning介绍](https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a?from=hackcv&hmsr=hackcv.com&utm_medium=hackcv.com&utm_source=hackcv.com)

> 元学习是解决“学习如何学习”的问题，现在做机器学习都是基于数据集进行训练，而无法有效利用已有的知识，如何让机器学习“举一反三”，是这个领域要解决的问题。本文作者在自然语言处理上有很丰富的元学习模型经验，本文是对这一路科研的介绍和总结，并使用PyTorch从零构建了一个元学习模型，并分享了其中的一些经验。

- [谷歌发布MobileNetV2：下一代端上计算机视觉网络](https://research.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html?from=hackcv&hmsr=hackcv.com&utm_medium=hackcv.com&utm_source=hackcv.com)

> 谷歌发布MobileNet第二个版本，通过添加捷径的方式，让训练和识别速度更快，与第一版相比，准确率也有提升。

- [转换器Transformer的注释版本](http://nlp.seas.harvard.edu/2018/04/03/attention.html?from=hackcv&hmsr=hackcv.com&utm_medium=hackcv.com&utm_source=hackcv.com)

> 谷歌的论文Attention_is_All_You_Need中的Transformer转换器在转换质量上有较大提升，为自然语言处理的很多任务提供了一个新架构。本文是作者按照论文进行一行一行的代码实现，添加了详细的注释，非常赞！

